<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>clemcore.clemgame.metrics API documentation</title>
<meta name="description" content="Definition of metrics/scores that should be defined and logged for all games.
This constants should be used so that the naming is standardised across …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>clemcore.clemgame.metrics</code></h1>
</header>
<section id="section-intro">
<p>Definition of metrics/scores that should be defined and logged for all games.
This constants should be used so that the naming is standardised across games.</p>
<p>Important: If the game is aborted, all episode-level scores must be set to numpy.nan
and turn-level scores can be computed for the valid turns before the abortion action.</p>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="clemcore.clemgame.metrics.BENCH_SCORE"><code class="name">var <span class="ident">BENCH_SCORE</span></code></dt>
<dd>
<div class="desc"><p>The main score of the game. It is a value between 0 and 100 that summarises
the overall performance of a game play.</p>
<p>Should be np.nan if the game was aborted.
Record level: episode</p></div>
</dd>
<dt id="clemcore.clemgame.metrics.METRIC_ABORTED"><code class="name">var <span class="ident">METRIC_ABORTED</span></code></dt>
<dd>
<div class="desc"><p>At the episode level, either 0 or 1 whether the game play has been aborted (1) or not (0)
(due to violation of the game rules e.g. not parsable response or re-prompt for n turns))
(this metric does not include games lost).
Record level: episode</p></div>
</dd>
<dt id="clemcore.clemgame.metrics.METRIC_LOSE"><code class="name">var <span class="ident">METRIC_LOSE</span></code></dt>
<dd>
<div class="desc"><p>At the episode level, either 0 or 1 whether the game play has been lost (1) or not (0)
(this metric does not include aborted games; the game is lost, when the game goal is not reached
within the declared number of max_turns, in this sense it’s the opposite of success).</p>
<p>This is always 0 if the game was aborted.</p>
<p>Record level: episode</p></div>
</dd>
<dt id="clemcore.clemgame.metrics.METRIC_PLAYED"><code class="name">var <span class="ident">METRIC_PLAYED</span></code></dt>
<dd>
<div class="desc"><p>1 - ABORTED
This is computed and used by the eval scripts, which infer the % played from the aborted
score. This metric should not be implemented/stored for new games if the given eval
scripts are used, to avoid duplicates.
Record level: episode</p></div>
</dd>
<dt id="clemcore.clemgame.metrics.METRIC_REQUEST_COUNT"><code class="name">var <span class="ident">METRIC_REQUEST_COUNT</span></code></dt>
<dd>
<div class="desc"><p>How many requests to API calls have been made during the whole game play.
Record level: episode (and optionally also turn)</p></div>
</dd>
<dt id="clemcore.clemgame.metrics.METRIC_REQUEST_COUNT_PARSED"><code class="name">var <span class="ident">METRIC_REQUEST_COUNT_PARSED</span></code></dt>
<dd>
<div class="desc"><p>How many requests to API calls have been made during the whole game play that
could be successfully parsed.
Record level: episode (and optionally also turn)</p></div>
</dd>
<dt id="clemcore.clemgame.metrics.METRIC_REQUEST_COUNT_VIOLATED"><code class="name">var <span class="ident">METRIC_REQUEST_COUNT_VIOLATED</span></code></dt>
<dd>
<div class="desc"><p>How many requests to API calls have been made during the whole game play that
could NOT be succesfully parsed.
Record level: episode (and optionally also turn)</p></div>
</dd>
<dt id="clemcore.clemgame.metrics.METRIC_REQUEST_SUCCESS"><code class="name">var <span class="ident">METRIC_REQUEST_SUCCESS</span></code></dt>
<dd>
<div class="desc"><p>METRIC_REQUEST_COUNT_PARSED / METRIC_REQUEST_COUNT
Record level: episode (and optionally also turn)</p></div>
</dd>
<dt id="clemcore.clemgame.metrics.METRIC_SUCCESS"><code class="name">var <span class="ident">METRIC_SUCCESS</span></code></dt>
<dd>
<div class="desc"><p>At the episode level, either 0 or 1 whether the game play has been successful (1) or not (0)
(this metric does not include aborted games; the game is successful, when the game goal is reached
within the declared number of max_turns, in this sense it’s the opposite of lost).</p>
<p>This is always 0 if the game was aborted.</p>
<p>Record level: episode</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="clemcore.clemgame.metrics.GameScorer"><code class="flex name class">
<span>class <span class="ident">GameScorer</span></span>
<span>(</span><span>name: str, experiment: Dict, game_instance: Dict)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GameScorer:
    &#34;&#34;&#34;Calculates scores based on interaction logs.&#34;&#34;&#34;

    def __init__(self, name: str, experiment: Dict, game_instance: Dict):
        &#34;&#34;&#34;
        Args:
            name: The name of the game.
            experiment: The experiment to score.
            game_instance: The game instance to score.
        &#34;&#34;&#34;
        self.game_name = name
        self.experiment = experiment
        self.game_instance = game_instance
        &#34;&#34;&#34; Stores values of score computation &#34;&#34;&#34;
        self.scores = {
            &#34;turn scores&#34;: {},
            &#34;episode scores&#34;: {},
        }

    # mapworld games use this method directly ... because they overwrite store_scores to store images
    # we should maybe add an on_store_scores hook for this already providing the full path to the episode dir
    def store_results_file(self, data, file_name: str, dialogue_pair: str,
                           sub_dir: str = None, results_dir: str = None):
        store_results_file(self.game_name, data, file_name,
                           dialogue_pair=dialogue_pair,
                           sub_dir=sub_dir,
                           results_dir=results_dir)

    def store_scores(self, results_root: str, dialogue_pair: str, game_record_dir: str):
        &#34;&#34;&#34;Store calculated scores to scores.json file.
        Args:
            results_root: The root path to the results directory.
            dialogue_pair: A string path to the Player pair results directory.
            game_record_dir: The game&#39;s record directory path.
        &#34;&#34;&#34;
        store_results_file(self.game_name, self.scores, &#34;scores.json&#34;,
                           dialogue_pair=dialogue_pair,
                           sub_dir=game_record_dir,
                           results_dir=results_root)

    def log_turn_score(self, turn_idx, score_name, score_value):
        &#34;&#34;&#34;Record a turn-level score for a single turn.
        Args:
            turn_idx: The turn index for the turn the score is to be recorded for.
            score_name: The name of the turn-level score to record.
            score_value: The value to be recorded for the turn-level score for this turn.
        &#34;&#34;&#34;
        if isinstance(score_value, bool):
            module_logger.warning(f&#34;{self.game_name}: Score {score_name} value is boolean, this can break the eval!&#34;)
        if turn_idx not in self.scores[&#34;turn scores&#34;]:
            self.scores[&#34;turn scores&#34;][turn_idx] = {}
        if score_name in self.scores[&#34;turn scores&#34;][turn_idx]:
            module_logger.warning(f&#34;{self.game_name}: Score {score_name} overwritten at turn {turn_idx}!&#34;)
        self.scores[&#34;turn scores&#34;][turn_idx][score_name] = score_value
        module_logger.info(f&#34;{self.game_name}: Logged turn {turn_idx} score {score_name}={score_value}.&#34;)

    def log_episode_score(self, score_name, score_value):
        &#34;&#34;&#34;Record an episode-level score for a single turn.
        Args:
            score_name: The name of the episode-level score to record.
            score_value: The value to be recorded for the episode-level score.
        &#34;&#34;&#34;
        if score_name in self.scores[&#34;episode scores&#34;]:
            module_logger.warning(f&#34;{self.game_name}: Episode score {score_name} overwritten!&#34;)
        self.scores[&#34;episode scores&#34;][score_name] = score_value
        module_logger.info(f&#34;{self.game_name}: Logged episode score {score_name}={score_value}.&#34;)

    def compute_scores(self, episode_interactions: Dict) -&gt; None:
        &#34;&#34;&#34;Compute and log scores for a game episode.
        This method is used to perform complete scoring of an episode.
        Args:
            episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
                a benchmark run.
        &#34;&#34;&#34;
        self.score_turns(episode_interactions)
        self.score_game(episode_interactions)

    def score_turns(self, episode_interactions: Dict) -&gt; None:
        &#34;&#34;&#34;Iterate over episode turns, calculate and log turn-level scores.
        This method is intended to contain any game-specific turn-level scoring. Must be implemented!
        Use the log_turn_score method to log turn-level scores.
        Args:
            episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
                a benchmark run.
        &#34;&#34;&#34;
        # Loop over turns, calculate and log turn-specific scores
        raise NotImplementedError()

    def score_game(self, episode_interactions: Dict) -&gt; None:
        &#34;&#34;&#34;Calculate and record standard clembench metric scores for an episode.
        Args:
            episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
                a benchmark run.
        &#34;&#34;&#34;
        self.score_game_end(episode_interactions)
        self.score_requests(episode_interactions)
        self.log_main_score(episode_interactions)

    def score_game_end(self, episode_interactions: Dict) -&gt; None:
        &#34;&#34;&#34;Calculate and record the ABORTED, LOSE and SUCCESS standard clembench metric scores.
        Convenience method to cover mandatory clembench metric scores, so their calculation does not need to be
        implemented anew for each new clemgame.
        Args:
            episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
                a benchmark run.
        &#34;&#34;&#34;
        aborted = int(episode_interactions[METRIC_ABORTED])
        lose = int(episode_interactions[METRIC_LOSE]) if not aborted else 0
        success = 1 - lose if not aborted else 0

        self.log_episode_score(METRIC_ABORTED, aborted)
        self.log_episode_score(METRIC_LOSE, lose)
        self.log_episode_score(METRIC_SUCCESS, success)

    def score_requests(self, episode_interactions: Dict):
        &#34;&#34;&#34;Calculate and record standard request-based clembench metric scores.
        Records total request count, parsed, violated, and success ratio of parsed requests over all requests in an
        episode.
        Convenience method to cover mandatory clembench metric scores, so their calculation does not need to be
        implemented anew for each new clemgame.
        Args:
            episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
                a benchmark run.
        &#34;&#34;&#34;
        request_count = episode_interactions[
            METRIC_REQUEST_COUNT]  # could also be calculated by adding parsed and violated requests
        parsed_requests = episode_interactions[METRIC_REQUEST_COUNT_PARSED]
        violated_requests = episode_interactions[METRIC_REQUEST_COUNT_VIOLATED]

        self.log_episode_score(METRIC_REQUEST_COUNT, request_count)
        self.log_episode_score(METRIC_REQUEST_COUNT_PARSED, parsed_requests)
        self.log_episode_score(METRIC_REQUEST_COUNT_VIOLATED, violated_requests)
        self.log_episode_score(METRIC_REQUEST_SUCCESS, parsed_requests / request_count)

    def log_main_score(self, episode_interactions: Dict):
        &#34;&#34;&#34;Record the game&#39;s main score.
        Replace this method with a method that calculates and logs the clemgame&#39;s main score aka BENCH_SCORE.
        Must be implemented! Recording BENCH_SCORE is mandatory.
        Args:
            episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
                a benchmark run.
        &#34;&#34;&#34;
        raise NotImplementedError()</code></pre>
</details>
<div class="desc"><p>Calculates scores based on interaction logs.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>The name of the game.</dd>
<dt><strong><code>experiment</code></strong></dt>
<dd>The experiment to score.</dd>
<dt><strong><code>game_instance</code></strong></dt>
<dd>The game instance to score.</dd>
</dl></div>
<h3>Instance variables</h3>
<dl>
<dt id="clemcore.clemgame.metrics.GameScorer.game_instance"><code class="name">var <span class="ident">game_instance</span></code></dt>
<dd>
<div class="desc"><p>Stores values of score computation</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="clemcore.clemgame.metrics.GameScorer.compute_scores"><code class="name flex">
<span>def <span class="ident">compute_scores</span></span>(<span>self, episode_interactions: Dict) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_scores(self, episode_interactions: Dict) -&gt; None:
    &#34;&#34;&#34;Compute and log scores for a game episode.
    This method is used to perform complete scoring of an episode.
    Args:
        episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
            a benchmark run.
    &#34;&#34;&#34;
    self.score_turns(episode_interactions)
    self.score_game(episode_interactions)</code></pre>
</details>
<div class="desc"><p>Compute and log scores for a game episode.
This method is used to perform complete scoring of an episode.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>episode_interactions</code></strong></dt>
<dd>Dict containing the episode's interactions. This contains the actions recorded during
a benchmark run.</dd>
</dl></div>
</dd>
<dt id="clemcore.clemgame.metrics.GameScorer.log_episode_score"><code class="name flex">
<span>def <span class="ident">log_episode_score</span></span>(<span>self, score_name, score_value)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_episode_score(self, score_name, score_value):
    &#34;&#34;&#34;Record an episode-level score for a single turn.
    Args:
        score_name: The name of the episode-level score to record.
        score_value: The value to be recorded for the episode-level score.
    &#34;&#34;&#34;
    if score_name in self.scores[&#34;episode scores&#34;]:
        module_logger.warning(f&#34;{self.game_name}: Episode score {score_name} overwritten!&#34;)
    self.scores[&#34;episode scores&#34;][score_name] = score_value
    module_logger.info(f&#34;{self.game_name}: Logged episode score {score_name}={score_value}.&#34;)</code></pre>
</details>
<div class="desc"><p>Record an episode-level score for a single turn.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>score_name</code></strong></dt>
<dd>The name of the episode-level score to record.</dd>
<dt><strong><code>score_value</code></strong></dt>
<dd>The value to be recorded for the episode-level score.</dd>
</dl></div>
</dd>
<dt id="clemcore.clemgame.metrics.GameScorer.log_main_score"><code class="name flex">
<span>def <span class="ident">log_main_score</span></span>(<span>self, episode_interactions: Dict)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_main_score(self, episode_interactions: Dict):
    &#34;&#34;&#34;Record the game&#39;s main score.
    Replace this method with a method that calculates and logs the clemgame&#39;s main score aka BENCH_SCORE.
    Must be implemented! Recording BENCH_SCORE is mandatory.
    Args:
        episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
            a benchmark run.
    &#34;&#34;&#34;
    raise NotImplementedError()</code></pre>
</details>
<div class="desc"><p>Record the game's main score.
Replace this method with a method that calculates and logs the clemgame's main score aka BENCH_SCORE.
Must be implemented! Recording BENCH_SCORE is mandatory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>episode_interactions</code></strong></dt>
<dd>Dict containing the episode's interactions. This contains the actions recorded during
a benchmark run.</dd>
</dl></div>
</dd>
<dt id="clemcore.clemgame.metrics.GameScorer.log_turn_score"><code class="name flex">
<span>def <span class="ident">log_turn_score</span></span>(<span>self, turn_idx, score_name, score_value)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log_turn_score(self, turn_idx, score_name, score_value):
    &#34;&#34;&#34;Record a turn-level score for a single turn.
    Args:
        turn_idx: The turn index for the turn the score is to be recorded for.
        score_name: The name of the turn-level score to record.
        score_value: The value to be recorded for the turn-level score for this turn.
    &#34;&#34;&#34;
    if isinstance(score_value, bool):
        module_logger.warning(f&#34;{self.game_name}: Score {score_name} value is boolean, this can break the eval!&#34;)
    if turn_idx not in self.scores[&#34;turn scores&#34;]:
        self.scores[&#34;turn scores&#34;][turn_idx] = {}
    if score_name in self.scores[&#34;turn scores&#34;][turn_idx]:
        module_logger.warning(f&#34;{self.game_name}: Score {score_name} overwritten at turn {turn_idx}!&#34;)
    self.scores[&#34;turn scores&#34;][turn_idx][score_name] = score_value
    module_logger.info(f&#34;{self.game_name}: Logged turn {turn_idx} score {score_name}={score_value}.&#34;)</code></pre>
</details>
<div class="desc"><p>Record a turn-level score for a single turn.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>turn_idx</code></strong></dt>
<dd>The turn index for the turn the score is to be recorded for.</dd>
<dt><strong><code>score_name</code></strong></dt>
<dd>The name of the turn-level score to record.</dd>
<dt><strong><code>score_value</code></strong></dt>
<dd>The value to be recorded for the turn-level score for this turn.</dd>
</dl></div>
</dd>
<dt id="clemcore.clemgame.metrics.GameScorer.score_game"><code class="name flex">
<span>def <span class="ident">score_game</span></span>(<span>self, episode_interactions: Dict) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score_game(self, episode_interactions: Dict) -&gt; None:
    &#34;&#34;&#34;Calculate and record standard clembench metric scores for an episode.
    Args:
        episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
            a benchmark run.
    &#34;&#34;&#34;
    self.score_game_end(episode_interactions)
    self.score_requests(episode_interactions)
    self.log_main_score(episode_interactions)</code></pre>
</details>
<div class="desc"><p>Calculate and record standard clembench metric scores for an episode.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>episode_interactions</code></strong></dt>
<dd>Dict containing the episode's interactions. This contains the actions recorded during
a benchmark run.</dd>
</dl></div>
</dd>
<dt id="clemcore.clemgame.metrics.GameScorer.score_game_end"><code class="name flex">
<span>def <span class="ident">score_game_end</span></span>(<span>self, episode_interactions: Dict) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score_game_end(self, episode_interactions: Dict) -&gt; None:
    &#34;&#34;&#34;Calculate and record the ABORTED, LOSE and SUCCESS standard clembench metric scores.
    Convenience method to cover mandatory clembench metric scores, so their calculation does not need to be
    implemented anew for each new clemgame.
    Args:
        episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
            a benchmark run.
    &#34;&#34;&#34;
    aborted = int(episode_interactions[METRIC_ABORTED])
    lose = int(episode_interactions[METRIC_LOSE]) if not aborted else 0
    success = 1 - lose if not aborted else 0

    self.log_episode_score(METRIC_ABORTED, aborted)
    self.log_episode_score(METRIC_LOSE, lose)
    self.log_episode_score(METRIC_SUCCESS, success)</code></pre>
</details>
<div class="desc"><p>Calculate and record the ABORTED, LOSE and SUCCESS standard clembench metric scores.
Convenience method to cover mandatory clembench metric scores, so their calculation does not need to be
implemented anew for each new clemgame.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>episode_interactions</code></strong></dt>
<dd>Dict containing the episode's interactions. This contains the actions recorded during
a benchmark run.</dd>
</dl></div>
</dd>
<dt id="clemcore.clemgame.metrics.GameScorer.score_requests"><code class="name flex">
<span>def <span class="ident">score_requests</span></span>(<span>self, episode_interactions: Dict)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score_requests(self, episode_interactions: Dict):
    &#34;&#34;&#34;Calculate and record standard request-based clembench metric scores.
    Records total request count, parsed, violated, and success ratio of parsed requests over all requests in an
    episode.
    Convenience method to cover mandatory clembench metric scores, so their calculation does not need to be
    implemented anew for each new clemgame.
    Args:
        episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
            a benchmark run.
    &#34;&#34;&#34;
    request_count = episode_interactions[
        METRIC_REQUEST_COUNT]  # could also be calculated by adding parsed and violated requests
    parsed_requests = episode_interactions[METRIC_REQUEST_COUNT_PARSED]
    violated_requests = episode_interactions[METRIC_REQUEST_COUNT_VIOLATED]

    self.log_episode_score(METRIC_REQUEST_COUNT, request_count)
    self.log_episode_score(METRIC_REQUEST_COUNT_PARSED, parsed_requests)
    self.log_episode_score(METRIC_REQUEST_COUNT_VIOLATED, violated_requests)
    self.log_episode_score(METRIC_REQUEST_SUCCESS, parsed_requests / request_count)</code></pre>
</details>
<div class="desc"><p>Calculate and record standard request-based clembench metric scores.
Records total request count, parsed, violated, and success ratio of parsed requests over all requests in an
episode.
Convenience method to cover mandatory clembench metric scores, so their calculation does not need to be
implemented anew for each new clemgame.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>episode_interactions</code></strong></dt>
<dd>Dict containing the episode's interactions. This contains the actions recorded during
a benchmark run.</dd>
</dl></div>
</dd>
<dt id="clemcore.clemgame.metrics.GameScorer.score_turns"><code class="name flex">
<span>def <span class="ident">score_turns</span></span>(<span>self, episode_interactions: Dict) ‑> None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def score_turns(self, episode_interactions: Dict) -&gt; None:
    &#34;&#34;&#34;Iterate over episode turns, calculate and log turn-level scores.
    This method is intended to contain any game-specific turn-level scoring. Must be implemented!
    Use the log_turn_score method to log turn-level scores.
    Args:
        episode_interactions: Dict containing the episode&#39;s interactions. This contains the actions recorded during
            a benchmark run.
    &#34;&#34;&#34;
    # Loop over turns, calculate and log turn-specific scores
    raise NotImplementedError()</code></pre>
</details>
<div class="desc"><p>Iterate over episode turns, calculate and log turn-level scores.
This method is intended to contain any game-specific turn-level scoring. Must be implemented!
Use the log_turn_score method to log turn-level scores.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>episode_interactions</code></strong></dt>
<dd>Dict containing the episode's interactions. This contains the actions recorded during
a benchmark run.</dd>
</dl></div>
</dd>
<dt id="clemcore.clemgame.metrics.GameScorer.store_results_file"><code class="name flex">
<span>def <span class="ident">store_results_file</span></span>(<span>self,<br>data,<br>file_name: str,<br>dialogue_pair: str,<br>sub_dir: str = None,<br>results_dir: str = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def store_results_file(self, data, file_name: str, dialogue_pair: str,
                       sub_dir: str = None, results_dir: str = None):
    store_results_file(self.game_name, data, file_name,
                       dialogue_pair=dialogue_pair,
                       sub_dir=sub_dir,
                       results_dir=results_dir)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="clemcore.clemgame.metrics.GameScorer.store_scores"><code class="name flex">
<span>def <span class="ident">store_scores</span></span>(<span>self, results_root: str, dialogue_pair: str, game_record_dir: str)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def store_scores(self, results_root: str, dialogue_pair: str, game_record_dir: str):
    &#34;&#34;&#34;Store calculated scores to scores.json file.
    Args:
        results_root: The root path to the results directory.
        dialogue_pair: A string path to the Player pair results directory.
        game_record_dir: The game&#39;s record directory path.
    &#34;&#34;&#34;
    store_results_file(self.game_name, self.scores, &#34;scores.json&#34;,
                       dialogue_pair=dialogue_pair,
                       sub_dir=game_record_dir,
                       results_dir=results_root)</code></pre>
</details>
<div class="desc"><p>Store calculated scores to scores.json file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>results_root</code></strong></dt>
<dd>The root path to the results directory.</dd>
<dt><strong><code>dialogue_pair</code></strong></dt>
<dd>A string path to the Player pair results directory.</dd>
<dt><strong><code>game_record_dir</code></strong></dt>
<dd>The game's record directory path.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="clemcore.clemgame" href="index.html">clemcore.clemgame</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="clemcore.clemgame.metrics.BENCH_SCORE" href="#clemcore.clemgame.metrics.BENCH_SCORE">BENCH_SCORE</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.METRIC_ABORTED" href="#clemcore.clemgame.metrics.METRIC_ABORTED">METRIC_ABORTED</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.METRIC_LOSE" href="#clemcore.clemgame.metrics.METRIC_LOSE">METRIC_LOSE</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.METRIC_PLAYED" href="#clemcore.clemgame.metrics.METRIC_PLAYED">METRIC_PLAYED</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.METRIC_REQUEST_COUNT" href="#clemcore.clemgame.metrics.METRIC_REQUEST_COUNT">METRIC_REQUEST_COUNT</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.METRIC_REQUEST_COUNT_PARSED" href="#clemcore.clemgame.metrics.METRIC_REQUEST_COUNT_PARSED">METRIC_REQUEST_COUNT_PARSED</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.METRIC_REQUEST_COUNT_VIOLATED" href="#clemcore.clemgame.metrics.METRIC_REQUEST_COUNT_VIOLATED">METRIC_REQUEST_COUNT_VIOLATED</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.METRIC_REQUEST_SUCCESS" href="#clemcore.clemgame.metrics.METRIC_REQUEST_SUCCESS">METRIC_REQUEST_SUCCESS</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.METRIC_SUCCESS" href="#clemcore.clemgame.metrics.METRIC_SUCCESS">METRIC_SUCCESS</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="clemcore.clemgame.metrics.GameScorer" href="#clemcore.clemgame.metrics.GameScorer">GameScorer</a></code></h4>
<ul class="two-column">
<li><code><a title="clemcore.clemgame.metrics.GameScorer.compute_scores" href="#clemcore.clemgame.metrics.GameScorer.compute_scores">compute_scores</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.GameScorer.game_instance" href="#clemcore.clemgame.metrics.GameScorer.game_instance">game_instance</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.GameScorer.log_episode_score" href="#clemcore.clemgame.metrics.GameScorer.log_episode_score">log_episode_score</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.GameScorer.log_main_score" href="#clemcore.clemgame.metrics.GameScorer.log_main_score">log_main_score</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.GameScorer.log_turn_score" href="#clemcore.clemgame.metrics.GameScorer.log_turn_score">log_turn_score</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.GameScorer.score_game" href="#clemcore.clemgame.metrics.GameScorer.score_game">score_game</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.GameScorer.score_game_end" href="#clemcore.clemgame.metrics.GameScorer.score_game_end">score_game_end</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.GameScorer.score_requests" href="#clemcore.clemgame.metrics.GameScorer.score_requests">score_requests</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.GameScorer.score_turns" href="#clemcore.clemgame.metrics.GameScorer.score_turns">score_turns</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.GameScorer.store_results_file" href="#clemcore.clemgame.metrics.GameScorer.store_results_file">store_results_file</a></code></li>
<li><code><a title="clemcore.clemgame.metrics.GameScorer.store_scores" href="#clemcore.clemgame.metrics.GameScorer.store_scores">store_scores</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
