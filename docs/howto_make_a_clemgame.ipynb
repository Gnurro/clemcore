{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "***This notebooks assumes familiarity with the documentation on [how to add new games](howto_add_games.md) and how to [log events and build records](logging_and_scoring.md). Please read those first. Also first have a look at [how to prototype games](https://github.com/clp-research/clembench/blob/main/docs/howto_prototype_games.ipynb), which shows the basic commands to interact with the LLMs.***",
   "id": "64344dd637365768"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Making a new clemgame",
   "id": "a662378a74a57d91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Index\n",
    "[Game concept](#game-concept)\n",
    "\n",
    "[Clemgame components](#clemgame-components)\n",
    "\n",
    "[Clemgame template and workspace setup](#clemgame-template-and-workspace-setup)\n",
    "\n",
    "[Instances and resources basics](#instances-basics-and-game-resources)\n",
    "\n",
    "[Player basics]\n",
    "\n",
    "[GameMaster implementation]\n",
    "\n",
    "[GameBenchmark implementation]\n",
    "\n",
    "[Testing and refining]"
   ],
   "id": "443993a208f339b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The core parts of a clemgame are separated into classes:\n",
    "- **Player** class handles players of the game, holding their message history and making calls to a backend model to generate responses.\n",
    "- **GameMaster** class handles the game loop, determining what each player gets prompted with when\n",
    "- **GameScorer** class handles scoring based on episode records\n",
    "- **GameBenchMark** class coordinates benchmark runs of the clemgame, initializing GameMaster with game instances and GameScorer with recorded episodes"
   ],
   "id": "552469143939fea9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Game concept\n",
    "The first step of creating a new clemgame is to come up with a game concept, or to find an existing dialogue game to adapt. In either case, it is important that the rules of the game can be implemented programmatically. While you can implement a clemgame with complex parsing, it is recommended to keep rules simple enough to understand benchmark results without learning code intricacies of the clemgame implementation."
   ],
   "id": "618c0b497c287bfd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Example game concept\n",
    "Let's implement a simple two-player game we will call `firstlast`.\n",
    "\n",
    "The players should engage in a back-and-forth conversation about a predefined topic. Player A starts with an utterance whose first and last word must start with a predefined letter, say  `d`. Player B must then reply with an utterance whose first and last word must be the next one in the alphabet (here, an `e`). And so on, for `n` rounds (where each round comprises a single turn, with an utterance from A or B). If an utterance does not conform to these rules (i.e. it is incorrect), the players lose the game. We also define a move format rule: If an utterance does not start with `I SAY: ` (i.e., it is invalid), the game is immediately aborted. If all utterances up to turn `n` are valid and correct, the game is successful.\n",
    "\n",
    "For instance, if the topic is `birds`, the initial letter is `h` and the number of ~~turns~~ rounds is 2, this would be a successful game:\n",
    "\n",
    "- Player A: \"Hi! I love birds, but it's hard to identify them. I need help.\" (h: hi / help)\n",
    "- Player B: \"I know what you mean. I can try to help, please describe it.\" (i: I/ it)\n",
    "- Player A: \"Just a moment... Ok, it's blue but looks like an Eurasian jay.\" (j: just / jay)\n",
    "- Player B: \"Kick in more details, otherwise I don't know.\" (k: kick / know)\n",
    "\n",
    "In each round, we need to check two aspects:\n",
    "- Does the utterance have a valid form fit to be parsed? Failing to meet this leads to the game being aborted. (Move format rule.)\n",
    "- Does the utterance fulfil the game rules for a successful round? Failing to meet this leads to game being lost. (Game rules.)\n",
    "\n",
    "To make things simple, in this example we will check only these conditions:\n",
    "- Validity: Does the utterance start with `I SAY:`?\n",
    "- Correctness: Do the first and last words begin with the same, correct letter at play?\n"
   ],
   "id": "5fc8d8e04b80da44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Clemgame components\n",
    "A clemgame needs at least the following components:\n",
    "\n",
    "1. [Game resources](#instances-basics-and-game-resources): All data, prompt templates and other files that are necessary to create instances of a game and to group these instances into experiments.\n",
    "2. [Instances](#instances-basics-and-game-resources): A JSON file containing the configuration of each instance of this game, grouped into experiments. This must be done by a script named `instancegenerator.py`, with a class that inherits from `GameInstanceGenerator`.\n",
    "3. [Players](#player-basics): A script that defines the programmatic behaviour and any other attributes of a player, inheriting from the `Player` base class. (This can be implemented in a file named `players.py`.)\n",
    "4. [Game Master](#gamemaster-implementation): A script that controls and enforces the defined move and game rules and dynamics, inheriting from the `GameMaster` base class. This must be implemented in the `master.py` file.\n",
    "5. [Game Benchmark](#gamebenchmark-implementation): A class that realises the game's running behavior for benchmarking, inheriting from `GameBenchmark`. This can also live in the file `master.py`.\n",
    "6. [Game registry](#game-registry) entry: A JSON-format specification of the clemgame required by the `clemcore` framework to locate the clemgame's files.\n",
    "\n",
    "Let's walk through the implementation step by step. We'll write the contents of `instancegenerator.py`, `master.py` and `players.py`, that you should save as files to run the game.\n",
    "\n",
    "**Note**: For the mandatory methods, always check the parent class documentation to be sure about the required and optional arguments."
   ],
   "id": "a9a4d58dbab06d9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Clemgame template and workspace setup\n",
    "The most convenient way to create a clemgame is to start with the template repository. It contains a complete implementation of the `taboo` clemgame as a base to work from.\n",
    "\n",
    "To start, follow these steps:\n",
    "1. Clone the clemgame template repository from https://github.com/clembench/clemgame-template . The directory you clone it to will be the workspace directory.\n",
    "2. Set up a python virtual environment with `clemcore`, as described in the [template readme](https://github.com/clembench/clemgame-template/blob/main/README.md).\n",
    "3. Inspect the template's files and structure.\n",
    "4. Create a directory for your clemgame, either copying the `taboo` directory and its contents, and renaming it for your game or by making a new directory. For the example game, create a `firstlast` directory ((and copy the contents of the `clembench/taboo` directory into it)).\n",
    "\n",
    "The example implementation below is based on this template.\n",
    "Run the cell below to set up paths for following code cells."
   ],
   "id": "4eb366c500f2791f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "# example game name\n",
    "GAME_NAME = 'firstlast'\n",
    "\n",
    "# set game path assuming that this notebook is in the workspace directory\n",
    "path = Path(f'{GAME_NAME}')"
   ],
   "id": "15e3574ab0a62066"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Instances basics and game resources\n",
    "Instances of the `taboo` clemgame are located under `clembench/taboo/in` in the `instances.json` file. All clemgames must follow this directory structure, with the clemgame's base directory containing a subdirectory named `in` which contains the `instances.json` file.\n",
    "\n",
    "The `instances.json` file contains a JSON object with the single key `experiments`, which holds a list of JSON objects, each corresponding to an experiment. An experiment holds clemgame parameters that apply for all its instances, and is used to evaluate and compare scores between different variants of a clemgame. The keys `name`, to identify different experiments, and `game_instances` are mandatory for all experiment objects.\n",
    "\n",
    "An experiment object's `game_instances` key holds a list of the experiment's instances, each containing different data that is needed by the Game Master (explained below) to play an individual episode of the clemgame.\n",
    "## Game resources\n",
    "Resources - located in the `resources` subdirectory - are files that are accessed for instance generation or by the Game Master running the clemgame.\n",
    "\n",
    "Conventionally, initial prompts that explain the game and its rules, and contain the starting state for an episode, are stored as `.template` files in `resources/initial_prompts`. Clemcore provides convenient methods to load these `.template` files (see below).\n",
    "\n",
    "As you can see in the `taboo` files, initial prompt templates usually contain placeholder strings like `$TARGET_WORD$` that are replaced with instance values to create individual initial prompts for each instance.\n",
    "\n",
    "## Example clemgame instances and resources\n",
    "(In this example, defining an episode requires instantiating the initial prompts and 3 additional parameters: The topic, the letter for the first player and the number of turns for that game play.)\n",
    "\n",
    "In this example, an instance defines which initial prompts to use and 3 additional parameters: The topic, the letter for the first player and the number of rounds that need to be successfully played.\n",
    "\n",
    "### Defining prompts with game rules\n",
    "\n",
    "The players need to be instructed on what the rules of the game are, possibly with some examples, at the beginning of the game. For that, we must define the initial prompts passed to player A and player B.\n",
    "\n",
    "In the template text, we can define variables that will later be filled with our chosen values (here: topic, first letter, number of rounds). The prompts need to be adjusted for player A and B according to their roles. For example:\n",
    "\n",
    "Player A:\n",
    "```\"Let's play a game. You must have a conversation about $topic with your partner. Your first turn must start and end with words that begin with the letter $letter. The reply of your partner must be similar, with the letter that comes after $letter in the alphabet. Then it's your turn again with the next letter, and so on. You'll do it for $n_turns turns. Always start your utterance with I SAY: and then give your answer. If you break the rules, you lose.\"```\n",
    "\n",
    "Player B:\n",
    "```\"Let's play a game. You must have a conversation about $topic with your partner. Their first turn must start and end with words that begin with the letter $letter. Your reply must be similar, with the letter that comes after $letter in the alphabet. Then it's their turn again with the next letter, and so on. You'll do it for $n_turns turns. Always start your utterance with I SAY: and then give your answer. If you break the rules, you lose.\"```\n",
    "NOTE: These templates are assuming the use of `string.Template` to fill the placeholders (starting with `$`, like `$n_turns`).\n",
    "\n",
    "You can later refine these initial prompts based on how models handle them. Save the initial prompts as plain texts using `.template` as an extension in the `initial_prompts` directory. You can save many templates if you wish to test different prompts, and read each of them when you generate game instances (see below). Note that this template is just an example and has not been tested. (Decide what amount of prompt engineering you will do at this step. Once you are satisfied with your preliminary results (i.e. the model can process the instructions well enough for your purposes), save to file...)\n",
    "\n",
    "### Additional resources\n",
    "Everything else needed to create instances for the game or to be accessed by the game master should be saved into the `firstlast/resources` directory as well.\n",
    "\n",
    "Let's create a `topics.txt` file with a list of topics that we can later sample from to create our instances. The following cell will do so:"
   ],
   "id": "8cf1cd13cfa40d1b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "topics = ['dogs', 'cats', 'birds', 'trees']\n",
    "\n",
    "with open(path / 'resources' / 'topics.txt', 'w') as file:\n",
    "    for topic in topics:\n",
    "        file.write(topic + '\\n')"
   ],
   "id": "1c76137fd40a5bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can also write the topics manually using any text editor or IDE. Intended `topics.txt` content:\n",
    "```\n",
    "dogs\n",
    "cats\n",
    "birds\n",
    "trees\n",
    "```\n",
    "### Creating game instances\n",
    "((Remove the template `instancegenerator.py` and)) Create a Python script called `instancegenerator.py` in `firstlast/`. Running this file will create a `JSON` file in `firstlast/in/`, called `instances.json`. The clemcore framework provides base classes to organise this. All we need to do is write a class that inherits from the `GameInstanceGenerator` class and write its `on_generate` method according to our needs. Then, in the main call, instantiate this class and call its `.generate()` method.\n",
    "\n",
    "In the `on_generate` method, we define experiments and then define instances of each experiment. An instance is a configuration of one specific game play and an experiment is a set of related instances. We can define what is an experiment and what is an instance depending on what dimensions we want to evaluate later.\n",
    "\n",
    "For our game, let's define an experiment as a set of instances about the same topic. To define an instance in an experiment, we define the initial letter, the initial prompts and the number of turns. This is useful if we wish to evaluate the performance of LLMs on variations of the same topic. Another possibility would be to define experiment as a set of instances with the same number of turns, and then each instance could be about different topics. That's our choice.\n",
    "\n",
    "Running this will automatically create a `JSON` file with a key `experiments`, which is a list of experiments. Each element has a name and a list of `game_instances`. A game instance should have at least a `game_id` assigning an index to that instance and other keys and values that are necessary to play the game. Here, the initial prompts can have their slots already filled with the instance's values (or we can leave that for the `setup` method of the game master).\n",
    "\n",
    "Here is an example of the structure we need:\n",
    "\n",
    "```JSON\n",
    "{\n",
    "    \"experiments\": [\n",
    "        {\n",
    "            \"name\": \"NAME_1\",\n",
    "            \"game_instances\": [\n",
    "                {\n",
    "                    \"game_id\": 0,\n",
    "                    \"first_letter\": \"LETTER\",\n",
    "                    \"n_turns\": \"N\",\n",
    "                    \"prompt_player_a\": \"PROMPT_A\",\n",
    "                    \"prompt_player_b\": \"PROMPT_B\",\n",
    "                },\n",
    "                {\n",
    "                    \"game_id\": 1,\n",
    "                    \"first_letter\": \"LETTER\",\n",
    "                    \"n_turns\": \"N\",\n",
    "                    \"prompt_player_a\": \"PROMPT_A\",\n",
    "                    \"prompt_player_b\": \"PROMPT_B\",\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"NAME_2\",\n",
    "            \"game_instances\": [\n",
    "                {\n",
    "                    \"game_id\": 0,\n",
    "                    \"first_letter\": \"LETTER\",\n",
    "                    \"n_turns\": \"N\",\n",
    "                    \"prompt_player_a\": \"PROMPT_A\",\n",
    "                    \"prompt_player_b\": \"PROMPT_B\",\n",
    "                },\n",
    "                {\n",
    "                    \"game_id\": 1,\n",
    "                    \"first_letter\": \"LETTER\",\n",
    "                    \"n_turns\": \"N\",\n",
    "                    \"prompt_player_a\": \"PROMPT_A\",\n",
    "                    \"prompt_player_b\": \"PROMPT_B\",\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "***Note***: The `instances.json` file should contain everything that the game master needs to set up the configuration to play the game! We can add as many keys and values and we need.\n",
    "\n",
    "Here is an example:"
   ],
   "id": "c6758cac3014cc63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save the contents of this cell as games/firstlast/instancegenerator.py\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import logging\n",
    "\n",
    "from clemcore.clemgame import GameInstanceGenerator\n",
    "\n",
    "# initialize logging:\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# set the name of the game in the script, as you named the directory\n",
    "# this name will be used everywhere, including in the table of results\n",
    "GAME_NAME = 'firstlast'\n",
    "# we will create 10 instances for each experiment; vary this as you wish\n",
    "N_INSTANCES = 10\n",
    "# if the generation involves randomness, remember to set a random seed\n",
    "SEED = 123\n",
    "\n",
    "class FirstLastGameInstanceGenerator(GameInstanceGenerator):\n",
    "    def __init__(self):\n",
    "        # always do this to initialise GameInstanceGenerator\n",
    "        super().__init__(os.path.dirname(__file__))\n",
    "\n",
    "    # define on_generate, a mandatory method\n",
    "    def on_generate(self):\n",
    "        # get the list of topics, which will be our experiments\n",
    "        topics = self.load_file('resources/topics.txt').strip('\\n').split('\\n')\n",
    "        # get the prompts for player a and player b\n",
    "        # we'll keep the prompts fixed in all instances, replacing only the\n",
    "        # necessary slots (but you can do it differently)\n",
    "        prompt_a = self.load_template('resources/initial_prompts/initial_prompt_a')\n",
    "        prompt_b = self.load_template('resources/initial_prompts/initial_prompt_b')\n",
    "\n",
    "        # building the file, one experiment at a time\n",
    "        for topic in topics:\n",
    "            # create an experiment (for us, named after a topic)\n",
    "            experiment = self.add_experiment(topic)\n",
    "            # build N_INSTANCES instances for each experiment\n",
    "            for game_id in range(N_INSTANCES):\n",
    "                # set the parameters\n",
    "                # here we do it randomly, but that can also be read from a file\n",
    "                # one of the first 5 letters in the alphabet\n",
    "                letter = random.choice(string.ascii_lowercase[:5])\n",
    "                # up to 8 turns, so that we don't run out of letters\n",
    "                n_turns = random.randint(3, 8)\n",
    "                # create a game instance, using a game_id counter/index\n",
    "                instance = self.add_game_instance(experiment, game_id)\n",
    "                # populate the game instance with its parameters\n",
    "                instance['first_letter'] = letter\n",
    "                instance['n_turns'] = n_turns\n",
    "                instance['prompt_player_a'] = self.create_prompt(\n",
    "                    topic, prompt_a, letter, n_turns)\n",
    "                instance['prompt_player_b'] = self.create_prompt(\n",
    "                    topic, prompt_b, letter, n_turns)\n",
    "\n",
    "    # an additional method, specific for our example\n",
    "    def create_prompt(self,\n",
    "                      topic: str,\n",
    "                      prompt: str,\n",
    "                      letter: str,\n",
    "                      n_turns: int) -> str:\n",
    "        \"\"\"Replace a prompt template with slot values.\"\"\"\n",
    "        text = string.Template(prompt).substitute(topic=topic, letter=letter,\n",
    "                                                  n_turns=n_turns)\n",
    "        return text\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    random.seed(SEED)\n",
    "    # always call this, which will actually generate and save the JSON file\n",
    "    FirstLastGameInstanceGenerator().generate()\n"
   ],
   "id": "815b1bd2dc3e40b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Summary:\n",
    "\n",
    "- Write a class that inherits from ```GameInstanceGenerator```.\n",
    "- You must implement the ```on_generate``` method, which should call ```self.add_experiment()``` to add experiments and ```self.add_game_instance()``` to add instances. Populate the game instance with keys and values.\n",
    "- ```GameInstanceGenerator``` has methods to load various files inside the game directory, for example ```self.load_template()``` and  ```self.load_file()```.\n",
    "- In ```'__main__'```, call ```FirstLastGameInstanceGenerator().generate()```.\n",
    "- Set a random seed if your generation relies on randomness; when you need new instances, change the random seed.\n",
    "\n",
    "An ideal clemgame instance generation allows creating varied new sets of instances by simply running `instancegenerator.py` with a new seed value."
   ],
   "id": "21d1f9cf8bc3a9cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Player basics\n",
    "Players are handled by child classes of the clemcore `Player` class, which holds the message history of a player and communicates with the model backend. A programmatic response method for testing is also commonly set up according to the clemgame.\n",
    "## Defining the Player class\n",
    "Create a python file called `player.py` in the workspace directory. (If your player class/es are very compact, you might define them in `master.py` instead, the creation of which is covered [below](#gamemaster-implementation).)\n",
    "\n",
    "In our game, the role of player A and B are symmetric, i.e. they behave the same way and have the same tasks and goals. So we can define one class and instantiate both players from it. If in your game players have different roles, then define two types of Player objects. The only method that we must implement is `_custom_response()`, which must define a programmatic behaviour for this player. The rest (getting and generating utterances via API calls to LLMs) is taken care of by the framework (we'll see below how to use it). Of course, you can add more methods that relate to the behaviour of the player in your game.\n",
    "\n",
    "The programatic behaviour is useful in two cases: when the player is really a program (i.e. it sends only predefined messages, e.g. read from a file, not retrieved from an LLM agent) or for testing your program, using the `mock` setting that does not make API calls to any LLMs. For the first case, the argument `model_name` in the initialisation should be set to `\"programmatic\"`.\n",
    "\n",
    "We also initialise a list to represent the dialogue history of this player. It will be incrementally built during the game play by appending new utterances to it."
   ],
   "id": "eabca2044e7f5b18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# save the contents of this cell as firstlast/players.py\n",
    "\n",
    "import random\n",
    "import logging\n",
    "from string import ascii_lowercase as letters\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "from clemcore.clemgame import Player\n",
    "from clemcore.backends import Model\n",
    "\n",
    "# initialize logging:\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Speaker(Player):\n",
    "    def __init__(self, model: Model, letter: str, firstlast_player: str, name: str = None):\n",
    "        # if the player is a program and you don't want to make API calls to\n",
    "        # LLMS, use model_name=\"programmatic\"\n",
    "        # TODO: check how programmatic is passed with ModelSpec\n",
    "        super().__init__(model, name)\n",
    "        self.player: str = firstlast_player\n",
    "        self.initial_letter: str = letter\n",
    "\n",
    "    # implement this method as you prefer, with these same arguments\n",
    "    def _custom_response(self, context: Dict) -> str:\n",
    "        \"\"\"Return a mock message with the suitable letter and format.\n",
    "        Args:\n",
    "            context: The dialogue context to which the player should respond. Base class method, not used in this example.\n",
    "        Returns:\n",
    "            Mock message with the suitable letter and format.\n",
    "        \"\"\"\n",
    "        # get the first letter of the content of the last message\n",
    "        # messages is a list of dictionaries with messages in openai API format\n",
    "        turn_idx = len(self._messages)  # will be 1 if only initial prompt message is in message history\n",
    "\n",
    "        if turn_idx == 1 and self.player == 'A':\n",
    "            letter = 'I SAY: ' + self.initial_letter\n",
    "        else:\n",
    "            previous_letter = self._messages[-1]['content'][7].lower()\n",
    "            # introduce a small probability that the player fails\n",
    "            letter = self._sample_letter(previous_letter)\n",
    "        # return a string whose first and last tokens start with the next letter\n",
    "        return f\"{letter}xxx from {self.player}, turn {turn_idx} {letter.replace('I SAY: ', '')}xxx.\"\n",
    "\n",
    "    # an additional method specific for this game\n",
    "    # for testing, we want the utterances to be invalid or incorrect sometimes\n",
    "    def _sample_letter(self, letter: str) -> str:\n",
    "        \"\"\"Randomly decide which letter to use in a custom response message.\"\"\"\n",
    "        prob = random.random()\n",
    "        index = letters.index(letter)\n",
    "        if prob < 0.05:\n",
    "            # correct but invalid (no tag)\n",
    "            return letters[index + 1]\n",
    "        if prob < 0.1:\n",
    "            # valid tag but wrong letter\n",
    "            return 'I SAY: ' + letter\n",
    "        # valid and correct\n",
    "        return 'I SAY: ' + letters[index + 1]\n"
   ],
   "id": "bb1a6016486e280a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Summary:\n",
    "\n",
    "- Write a class that inherits from `Player`.\n",
    "- Define its `_custom_response()` method, which implements the programmatic behaviour of the player (for testing, or because it is really a program) and returns a string.\n",
    "- A `ModelSpec` defines the model to use, and the programmatic custom response is used by passing a `\"programmatic\"` `ModelSpec`."
   ],
   "id": "557b2d1b989a847f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GameMaster implementation\n",
    "The core clemgame functionality is implemented in its game master class, containing the play loop and handling any game-specific interaction. While a game master can be fully custom and based on just the minimal implementation in the clemcore `GameMaster` base class, the more extensive `DialogueGameMaster` base class inheriting from `GameMAster` is commonly used. `DialogueGameMaster` implements the game conversation loop described in the clembench papers, and only requires extension for game-specific behavior. The game master must be implemented in `master.py`, located in the root directory of a clemgame.\n",
    "## Defining the GameMaster class\n",
    "Create a python file named `master.py` in the `firstlast` directory.\n",
    "\n",
    "\n",
    "To define the game master, we need to write a class that inherits from `GameMaster` and implement its `setup()` and `play()` methods, which allow the episode to be created and run, and then its `compute_scores()` method that will compute the metrics that are required for evaluation. Score are computed after the game is finished, using the separate `score` argument in the cli script.\n",
    "\n",
    "To define the game master, we write a class that inherits from `DialogueGameMaster` and implement certain required methods, mainly its `_on_setup()` method and play loop methods like `_does_game_proceed()`.\n",
    "\n",
    "`setup()` and `play()` methods, which allow the episode to be created and run, and then its `compute_scores()` method that will compute the metrics that are required for evaluation. Score are computed after the game is finished, using the separate `score` argument in the cli script.\n",
    "// CHECK: now done with GameScorer, but there's compute_response_score() and compute_episode_score() in DGM base class now - are these for playpen or to be used for benchmark scoring?\n",
    "\n",
    "The metrics that every game must compute are listed at `clemcore/clemgame/metrics.py`, described in [log events and build records](logging_and_scoring.md) and in more detail in the paper's appendix. Note: You should **not** implement `METRIC_PLAYED` if you use the provided evaluation scripts, because this metric is inferred from `METRIC_ABORTED` there. Besides, any number of additional game-specific metrics can also be logged (see more below).\n",
    "\n",
    "These are the mandatory methods. However, for readability, we will also write auxiliary methods.\n",
    "\n",
    "**IMPORTANT**: The game master has to log ***every event*** that is relevant to reconstruct the interaction, build the transcript and evaluate the game.\n",
    "\n",
    "The `GameMaster` is also a `GameResourceLocator`, which has special methods to access and write files in the game's local directory, and a `GameRecorder`, which knows how to log events. We'll see below how to use it.\n",
    "\n",
    "The `GameMaster` and `DialogueGameMaster` base classes have a `GameResourceLocator` to access resources and a `GameRecorder` to be used by all involved class objects.\n",
    "### Initialisation\n",
    "\n",
    "The first step is to define how to initialise the game master. The `__init__` method gets the experiment object and a list of `ModelSpec` objects (`ModelSpec` objects are handled by the benchmark scripts). Initialise any needed attributes here. In our example, we define variables for whether the game gets aborted or lost and the number of actually completed game turns.\n",
    "\n",
    "```python\n",
    "import copy\n",
    "from typing import List, Dict, Tuple\n",
    "from string import ascii_lowercase as letters\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import clemcore.metrics as ms\n",
    "from clemcore.clemgame import DialogueGameMaster, GameBenchmark\n",
    "from clemcore.backends import Model\n",
    "\n",
    "from player import Speaker\n",
    "\n",
    "# initialize logging:\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "GAME_NAME = \"firstlast\"\n",
    "\n",
    "class FirstLast(DialogueGameMaster):\n",
    "    \"\"\"Implement mechanisms for playing FirstLast.\"\"\"\n",
    "    def __init__(self, game_name: str, game_path: str, experiment: Dict, player_models: List[Model]):\n",
    "        super().__init__(game_name, game_path, experiment, player_models)\n",
    "        # assign experiment attributes that will be necessary later\n",
    "        self.topic = experiment['name']\n",
    "````\n",
    "\n",
    "### Keeping records\n",
    "`DialogueGameMaster` handles all default record keeping, but game-specific records need to be implemented using the `log_key()` method.\n",
    "\n",
    "Note: see details about keeping records in [log events and build records](logging_and_scoring.md).\n",
    "#### Default record keeping\n",
    "All events that occur during the game, i.e. all actions by the game master and by the players, must be documented. This is done by the methods of `GameRecorder` of the `GameMaster`. The essential ones are:\n",
    "\n",
    "- At the beginning of every turn, call `log_next_turn()` (what is the definition of turn is up to you; here, it means one utterance by player A and one utterance by player B)\n",
    "- in the game setup, call `log_players()` in order to log the models that are playing this episode of the game\n",
    "- use `log_event()` to log all types of actions with a `to` and `from_`.\n",
    "- the `action` object passed to `log_event()` must contain at least a key `type` and a key `content`. The first can be send message, get message, metadata, parse, error, invalid format or any game-specific types. Content is the actual message to de displayed in the transcript.\n",
    "- use only the values 'Player 1', 'Player 2' or 'GM' for the `from_` and `to` arguments. Messages that the game master emits to itself should have 'GM' both in `from_` and `to`.\n",
    "- all events that involve making an API call should pass an additional `call` argument to `log_event()` containing the actual and exact API input and output objects, for posterior inspection if necessary.\n",
    "- any other object needed for scoring or documentation can be logged with `log_key()`.\n",
    "#### Record files\n",
    "Every action that is logged gets saved into the episode's `interactions.json` file after it is played. This file is then used to build game transcripts and to compute evaluation scores. The episode's `requests.json` file contains the API calls, saved when `log_event()` is called with a `call` argument. ((Make sure that you save the exact API input and output. -> should be done by default now)) If you use a list, make deep copies to guarantee that you are not logging an object that mutates.\n",
    "#### Records and scoring\n",
    "((Besides game events, the game master must also compute and log scores. Please read `logdoc.md` for details. In summary, there are two types of scores: episode-level and turn-level. These should be computed inside the method `compute_scores()` using `log_episode_score()` and `log_turn_score()`, respectively.\n",
    "\n",
    "`compute_scores()` gets `interactions.json` dictionary as argument, so every key and value that are necessary to compute scores should be logged into the interaction file. -> now done with GameScorer))\n",
    "\n",
    "### IMPORTANT: Inspecting the game records\n",
    "\n",
    "During development, always check the generated `interactions.json` and `requests.json` to make sure that the API calls are passing the correct structure and that the records are being correctly saved.\n",
    "\n",
    "`interactions.json` is built by the game master as a way to represent the actual interaction (with all its meta-events like parsing messages or checking game rules). This is used to create the transcripts, which are a user-friendly visualisation of the interaction. But remember that this does not reflect the actual API calls, this only reflects what the game master makes of the game!\n",
    "\n",
    "The actual prompts and responses from the model are saved into `requests.json`, when an action is logged with its corresponding prompt and response object (see below how to do it). This file will reflect what was actually passed to and from the LLM. Remeber that LLMS do not keep a internal state, so every call to a model must contain its full dialogue history. Also remember that when there are two LLMs playing at once, each will have its own dialogue history, which may be different! That's why, for debugging purposes, only looking at `interactions.json` is not enough, because it may not reflect exactly what the LLMs consumed and output.\n",
    "\n",
    "#### Logging framework level events\n",
    "To log framework level events for debugging and benchmarking, the framework standard logger is defined and used: `logger = get_logger(__name__)`. Everything logged this way is appended to `clembench.log`.\n",
    "\n",
    "### Episode setup\n",
    "\n",
    "The `_on_setup()` method gets all keys=values in the instance dictionary, as we defined above. Use this method to set up everything that is needed so that the game can be played.\n",
    "\n",
    "In our example, we instantiate both players (including adding the initial prompts to their message history), the initial letter and some variables to keep track of the (in)valid requests (which are mandatory scores).\n",
    "\n",
    "```python\n",
    "    def _on_setup(self, **game_instance) -> None:\n",
    "        \"\"\"Set up the episode (mandatory).\"\"\"\n",
    "        self.game_instance = game_instance\n",
    "\n",
    "        self.n_turns = game_instance['n_turns']\n",
    "\n",
    "        # instantiate both players:\n",
    "        self.player_a = Speaker(self.player_models[0], 'A', game_instance['first_letter'])\n",
    "        self.player_b = Speaker(self.player_models[1], 'B', game_instance['first_letter'])\n",
    "\n",
    "        # add players, including assigning their initial prompts:\n",
    "        self.add_player(self.player_a, initial_prompt=game_instance['prompt_player_a'])\n",
    "        self.add_player(self.player_b, initial_prompt=game_instance['prompt_player_b'])\n",
    "\n",
    "        # initialise game variables:\n",
    "        self.current_turn: int = 0\n",
    "        self.current_letter: str = game_instance['first_letter']\n",
    "\n",
    "        # initialise common metrics:\n",
    "        self.request_counts = [0] * (self.n_turns + 1)\n",
    "        self.parsed_request_counts = [0] * (self.n_turns + 1)\n",
    "        self.violated_request_counts = [0] * (self.n_turns + 1)\n",
    "\n",
    "        # initialise attributes that will be used for the evaluation scores\n",
    "        self.aborted: bool = False\n",
    "        self.lose: bool = False\n",
    "        self.complete_turns: int = 0\n",
    "\n",
    "        # log any additional keys that will be relevant for evaluation\n",
    "        self.log_key('n_turns', n_turns)\n",
    "```\n",
    "\n",
    "Summary:\n",
    "- The setup must define players and log other game-specific keys.\n"
   ],
   "id": "572117b4302ae731"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Playing the game\n",
    "The `DialogueGameMaster` base class comes with a play loop already implemented. It has a number of hook methods that are called inside of this play loop, and game specifics are to be implemented within these methods.\n",
    "\n",
    "This is the `play()` method of `DialogueGameMaster`:\n",
    "```python\n",
    "    def play(self) -> None:\n",
    "        \"\"\"Main play loop method.\n",
    "        This method is called to run the game for benchmarking.\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        while not done:\n",
    "            # get the current context message for the current player, set by set_context_for():\n",
    "            context = self.get_context_for(self.current_player)\n",
    "            # generate/get response from the player based on their message history and the passed context message:\n",
    "            response = self.current_player(context)\n",
    "            # pass the player response to the step() method for processing and determining if play continues:\n",
    "            done, _ = self.step(response)\n",
    "```\n",
    "The `step()` method makes method calls in this sequence:\n",
    "1. **compute_response_score(response, context)**: Calculate a turn-level score for this player response.\n",
    "2. get_response_feedback(response, context): Create textual feedback to the player response.\n",
    "3. **\\_validate_player_response(self.current_player, response)**: Decide if a player response is valid.\n",
    "4. **\\_parse_response(self.current_player, response)**: Decide if a response utterance should be modified and apply modifications.\n",
    "5. **\\_on_valid_player_response(self.current_player, parsed_response)**: Method executed after a player response has been parsed and validated.\n",
    "6. _should_pass_turn(): Determine if a player is done for the current round.\n",
    "7. _next_player(): The game master passes the turn to the next player in the player list (order as added).\n",
    "8. **_start_next_round()**: Start next round when we cycled through the whole list i.e. it is again the first player's turn.\n",
    "9. **\\_does_game_proceed()**: Check if game should proceed.\n",
    "10. _on_before_round(): Executed in the play loop before a new round of gameplay starts.\n",
    "11. **_on_after_round()**: Executed in the play loop after a round of gameplay finishes i.e. _start_next_round() resolves to True.\n",
    "12. **_on_after_game()**: Executed once at the end, after exiting the play loop.\n",
    "\n",
    "While all listed methods can be adapted for your game, we will implement adapted versions of the bold methods. Implementation of the bold methods (except `parse_response()`) is mandatory.\n"
   ],
   "id": "45105f91b7acf8bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### _validate_player_response()\n",
    "The `_validate_player_response()` method checks if the player's response conforms to the game's move format rules. For `firstlast`, we have a single move format rule: Player responses must start with `I SAY:`. If this rule is not followed the episode is aborted.\n",
    "\n",
    "```python\n",
    "    def _validate_player_response(self, player: Player, response: str) -> bool:\n",
    "        \"\"\"Check if the response follow the move format rule.\n",
    "        Args:\n",
    "            player: The player that produced the response.\n",
    "            response: The response string.\n",
    "        Returns:\n",
    "            True if the move format rule was followed, else False.\n",
    "        \"\"\"\n",
    "        if not response.startswith('I SAY:'):\n",
    "            self.aborted = True\n",
    "            return False\n",
    "        return True\n",
    "```"
   ],
   "id": "c5044949c0490f15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### _parse_response()\n",
    "The `_parse_response()` method processes the response further. We also add the response of the current player as the context message for the next player's round.\n",
    "```python\n",
    "    def _parse_response(self, player: Player, response: str) -> Tuple(str, str):\n",
    "        \"\"\"Split the response and return the first and last word.\"\"\"\n",
    "        if player == self.player_a:\n",
    "            self.set_context_for(self.player_b, response)\n",
    "        if player == self.guesser:\n",
    "            self.set_context_for(self.describer, response)\n",
    "        # remove the move format tag and split on whitespace:\n",
    "        words = response[7:].split()\n",
    "        return words[0], words[-1]\n",
    "```"
   ],
   "id": "fd1871ca27991dcc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### _on_valid_player_response()\n",
    "The `_on_valid_player_response()` method processes the parsed response, checking for following game rules and setting relevant attributes. If the letter rule isn't followed, the episode is lost.\n",
    "```python\n",
    "    def _on_valid_player_response(self, player: Player, parsed_response: Tuple[str, str]):\n",
    "        \"\"\"Check player response for game rule adherence and assign next letter.\"\"\"\n",
    "        first_word_correct_letter = parsed_response[0][0] == self.current_letter  # True if the first letter of the first word in the response is correct\n",
    "        last_word_correct_letter = parsed_response[0][0] == parsed_response[1][0]  # True if the first letters of the first and last word match\n",
    "        self.correct_response = first_word_correct_letter and last_word_correct_letter\n",
    "        if not self.correct_response:\n",
    "            self.lose = True\n",
    "            # log the fact that the game is now lost\n",
    "            action = {'type': 'parse',\n",
    "                      'content': f'{first_token}/{last_token} violates rules'}\n",
    "            self.log_event(from_='GM', to='GM', action=action)\n",
    "        else:\n",
    "            # log the fact that the answer was correct\n",
    "            action = {'type': 'parse',\n",
    "                      'content': f'{first_token}/{last_token} conforms to rules'}\n",
    "            self.log_event(from_='GM', to='GM', action=action)\n",
    "```"
   ],
   "id": "66118dadf6014bc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### compute_response_score()\n",
    "Since we need to check for the player following the rules to determine the response score, we will use the methods defined above here.\n",
    "```python\n",
    "    def compute_response_score(self, response: str, context: Dict):\n",
    "        \"\"\"Compute a score for a player response.\n",
    "        Args:\n",
    "            response: The player response string to be scored.\n",
    "            context: The context message that was added to the player message history to produce the response.\n",
    "        Returns:\n",
    "            1 if the firstlast game rules were followed, 0 otherwise.\n",
    "        \"\"\"\n",
    "        if self._validate_player_response(self.current_player, response):\n",
    "            parsed_response = self._parse_response(self.current_player, response)\n",
    "            self._on_valid_player_response(self.current_player, parsed_response) # this method sets self.correct_response to True or False\n",
    "            return 1 if self.correct_response else 0\n",
    "        else:\n",
    "            return 0\n",
    "```"
   ],
   "id": "17455c5051b07733"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### _does_game_proceed()\n",
    "The `_does_game_proceed()` method determines if the episode should continue. Episodes are ended if:\n",
    "- The required number of rounds has been reached, making the episode successful.\n",
    "- The move format rule was violated, aborting the episode.\n",
    "- The letter rule was violated, losing the episode.\n",
    "```python\n",
    "    def _does_game_proceed(self) -> bool:\n",
    "        \"\"\"Check if game should proceed.\"\"\"\n",
    "        return (self.current_turn < self.n_turns\n",
    "                and not self.aborted\n",
    "                and not self.lose)\n",
    "```\n"
   ],
   "id": "a214a650751d1d02"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### _start_next_round()\n",
    "Since we want the players to move on to the next letter in the alphabet each time they respond, but not interfere with the response scoring, we adapt the `_start_next_round()` method to always move on to the next round after a player response. This way the `_on_after_round()` method that changes the current letter is called between each player's turn/round/response.\n",
    "```python\n",
    "    def _start_next_round(self) -> bool:\n",
    "        \"\"\"Start next round after each player's turn.\n",
    "        Returns:\n",
    "            True, when to start a new round\n",
    "        \"\"\"\n",
    "        return True\n",
    "```"
   ],
   "id": "5e1d21df12f20776"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### _on_after_round()\n",
    "The `_on_after_round()` method is called after an entire round is done - in the case of this `firstlast` implementation, this is after a player has responded. We set the current letter to the next letter in the alphabet in this method.\n",
    "```python\n",
    "    def _on_after_round(self):\n",
    "        \"\"\"Updates the letter being played after a successful round.\"\"\"\n",
    "        # update the letter being played:\n",
    "        current_index = letters.index(self.current_letter)\n",
    "        self.current_letter = letters[current_index + 1]\n",
    "```"
   ],
   "id": "4cfbecf8c83ca600"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### _on_after_game()\n",
    "Finally, we record counts and variables that are used later for scoring with the `_on_after_game()` method.\n",
    "\n",
    "```python\n",
    "    def _on_after_game(self) -> None:\n",
    "        \"\"\"Aux to log variables needed for scoring (firstlast specific)\"\"\"\n",
    "        self.log_key('Played turns', self.current_turn)\n",
    "        self.log_key('Complete turns', self.complete_turns)\n",
    "        self.log_key(ms.METRIC_ABORTED, self.aborted)\n",
    "        self.log_key(ms.METRIC_LOSE, self.lose)\n",
    "        self.log_key(ms.METRIC_REQUEST_COUNT, self.request_counts)\n",
    "        self.log_key(ms.METRIC_REQUEST_COUNT_PARSED, self.parsed_request_counts)\n",
    "        self.log_key(ms.METRIC_REQUEST_COUNT_VIOLATED, self.violated_request_counts)\n",
    "\n",
    "```"
   ],
   "id": "6205620caad80844"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Full example game master\n",
    "The cell below shows the combined code for the firstlast game master."
   ],
   "id": "625f55c08c0c49f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import copy\n",
    "from typing import List, Dict, Tuple\n",
    "from string import ascii_lowercase as letters\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import clemcore.clemgame.metrics as ms\n",
    "from clemcore.clemgame import DialogueGameMaster, GameBenchmark\n",
    "from clemcore.backends import Model\n",
    "\n",
    "from player import Speaker\n",
    "\n",
    "\n",
    "class FirstLast(DialogueGameMaster):\n",
    "    \"\"\"Implement mechanisms for playing FirstLast.\"\"\"\n",
    "    def __init__(self, game_name: str, game_path: str, experiment: Dict, player_models: List[Model]):\n",
    "        super().__init__(game_name, game_path, experiment, player_models)\n",
    "        # assign experiment attributes that will be necessary later\n",
    "        self.topic = experiment['name']\n",
    "\n",
    "    def _on_setup(self, **game_instance) -> None:\n",
    "        \"\"\"Set up the episode (mandatory).\"\"\"\n",
    "        self.game_instance = game_instance\n",
    "\n",
    "        self.n_turns = game_instance['n_turns']\n",
    "\n",
    "        # instantiate both players:\n",
    "        self.player_a = Speaker(self.player_models[0], 'A', game_instance['first_letter'])\n",
    "        self.player_b = Speaker(self.player_models[1], 'B', game_instance['first_letter'])\n",
    "\n",
    "        # add players, including assigning their initial prompts:\n",
    "        self.add_player(self.player_a, initial_prompt=game_instance['prompt_player_a'])\n",
    "        self.add_player(self.player_b, initial_prompt=game_instance['prompt_player_b'])\n",
    "\n",
    "        # initialise game variables:\n",
    "        self.current_turn: int = 0\n",
    "        self.current_letter: str = game_instance['first_letter']\n",
    "\n",
    "        # initialise common metrics:\n",
    "        self.request_counts = [0] * (self.n_turns + 1)\n",
    "        self.parsed_request_counts = [0] * (self.n_turns + 1)\n",
    "        self.violated_request_counts = [0] * (self.n_turns + 1)\n",
    "\n",
    "        # initialise attributes that will be used for the evaluation scores\n",
    "        self.aborted: bool = False\n",
    "        self.lose: bool = False\n",
    "        self.complete_turns: int = 0\n",
    "\n",
    "        # log any additional keys that will be relevant for evaluation\n",
    "        self.log_key('n_turns', game_instance['n_turns'])\n",
    "\n",
    "    def _validate_player_response(self, player: Player, response: str) -> bool:\n",
    "        \"\"\"Check if the response follow the move format rule.\n",
    "        Args:\n",
    "            player: The player that produced the response.\n",
    "            response: The response string.\n",
    "        Returns:\n",
    "            True if the move format rule was followed, else False.\n",
    "        \"\"\"\n",
    "        # increase the number of API requests:\n",
    "        self.request_counts[self.current_turn] += 1\n",
    "        # check move format rule:\n",
    "        if not response.startswith('I SAY:'):\n",
    "            self.aborted = True\n",
    "            # log the abortion event\n",
    "            action = {'type': 'invalid format', 'content': 'abort'}\n",
    "            self.log_event(from_='GM', to='GM', action=action)\n",
    "            # increase the counter of requests that violate form rules\n",
    "            self.violated_request_counts[self.current_turn] += 1\n",
    "            return False\n",
    "        else:\n",
    "            # increase the counter of requests that conform to form rules\n",
    "            self.parsed_request_counts[self.current_turn] += 1\n",
    "            # log the event that the string was valid (no strange characters)\n",
    "            action = {'type': 'metadata', 'content': 'valid string'}\n",
    "            self.log_event(from_='GM', to='GM', action=action)\n",
    "        return True\n",
    "\n",
    "    def _parse_response(self, player: Player, response: str) -> Tuple[str, str]:\n",
    "        \"\"\"Split the response and return the first and last word.\"\"\"\n",
    "        if player == self.player_a:\n",
    "            self.set_context_for(self.player_b, response)\n",
    "        if player == self.guesser:\n",
    "            self.set_context_for(self.describer, response)\n",
    "        # remove the move format tag and split on whitespace:\n",
    "        words = response[7:].split()\n",
    "        return words[0], words[-1]\n",
    "\n",
    "    def _on_valid_player_response(self, player: Player, parsed_response: Tuple[str, str]):\n",
    "        \"\"\"Check player response for game rule adherence.\"\"\"\n",
    "        first_word_correct_letter = parsed_response[0][0] == self.current_letter  # True if the first letter of the first word in the response is correct\n",
    "        last_word_correct_letter = parsed_response[0][0] == parsed_response[1][0]  # True if the first letters of the first and last word match\n",
    "        self.correct_response = first_word_correct_letter and last_word_correct_letter\n",
    "        if not self.correct_response:\n",
    "            self.lose = True\n",
    "            # log the fact that the game is now lost\n",
    "            action = {'type': 'parse',\n",
    "                      'content': f'{parsed_response[0]}/{parsed_response[1]} violates rules'}\n",
    "            self.log_event(from_='GM', to='GM', action=action)\n",
    "        else:\n",
    "            # log the fact that the answer was correct\n",
    "            action = {'type': 'parse',\n",
    "                      'content': f'{parsed_response[0]}/{parsed_response[1]} conforms to rules'}\n",
    "            self.log_event(from_='GM', to='GM', action=action)\n",
    "\n",
    "    def compute_response_score(self, response: str, context: Dict):\n",
    "        \"\"\"Compute a score for a player response.\n",
    "        Args:\n",
    "            response: The player response string to be scored.\n",
    "            context: The context message that was added to the player message history to produce the response.\n",
    "        Returns:\n",
    "            1 if the firstlast game rules were followed, 0 otherwise.\n",
    "        \"\"\"\n",
    "        if self._validate_player_response(self.current_player, response):\n",
    "            parsed_response = self._parse_response(self.current_player, response)\n",
    "            self._on_valid_player_response(self.current_player, parsed_response) # this method sets self.correct_response to True or False\n",
    "            return 1 if self.correct_response else 0\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _does_game_proceed(self) -> bool:\n",
    "        \"\"\"Check if game should proceed.\"\"\"\n",
    "        return (self.current_turn < self.n_turns\n",
    "                and not self.aborted\n",
    "                and not self.lose)\n",
    "\n",
    "    def _start_next_round(self) -> bool:\n",
    "        \"\"\"Start next round after each player's turn.\n",
    "        Returns:\n",
    "            True\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def _on_after_round(self):\n",
    "        \"\"\"Updates the letter being played after a successful round.\"\"\"\n",
    "        # update the letter being played:\n",
    "        current_index = letters.index(self.current_letter)\n",
    "        self.current_letter = letters[current_index + 1]\n",
    "        # increment firstlast turns:\n",
    "        self.current_turn += 1\n",
    "\n",
    "    def _on_after_game(self) -> None:\n",
    "        \"\"\"Log variables needed for scoring.\"\"\"\n",
    "        # log a message informing that the game was successfully played:\n",
    "        if not self.aborted and not self.lose:\n",
    "            action = {'type': 'info', 'content': 'game successful'}\n",
    "            self.log_event(from_='GM', to='GM', action=action)\n",
    "\n",
    "        # log a final message saying that the game did come to an end:\n",
    "        action = {'type': 'info', 'content': 'end game'}\n",
    "        self.log_event(from_='GM', to='GM', action=action)\n",
    "\n",
    "        self.log_key('Played turns', self.current_turn)\n",
    "        self.log_key('Complete turns', self.complete_turns)\n",
    "        self.log_key(ms.METRIC_ABORTED, self.aborted)\n",
    "        self.log_key(ms.METRIC_LOSE, self.lose)\n",
    "        self.log_key(ms.METRIC_REQUEST_COUNT, self.request_counts)\n",
    "        self.log_key(ms.METRIC_REQUEST_COUNT_PARSED, self.parsed_request_counts)\n",
    "        self.log_key(ms.METRIC_REQUEST_COUNT_VIOLATED, self.violated_request_counts)\n",
    "\n"
   ],
   "id": "e11871d116ccf37a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GameScorer implementation\n",
    "Each clemgame needs a game scorer, a child class of the clemcore `GameScorer` base class. The game scorer reads episode records and calculates evaluation scores."
   ],
   "id": "b331dbd2a6013d61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def FirstLastScorer(GameScorer):\n",
    "    \"\"\"Scorer for the firstlast game.\"\"\"\n",
    "    def __init__(self, game_name: str, experiment: Dict, game_instance: Dict):\n",
    "        super().__init__(game_name, experiment, game_instance)\n",
    "\n",
    "    def compute_scores(self, episode_interactions: Dict) -> None:\n",
    "        \"\"\"Compute episode-level and turn-level scores (mandatory).\"\"\"\n",
    "        played_turns = episode_interactions['Played turns']\n",
    "        complete_turns = episode_interactions['Complete turns']\n",
    "        # turn 0 was only the initial prompts, so we disregard it here\n",
    "        reqs = episode_interactions[ms.METRIC_REQUEST_COUNT][1:]\n",
    "        p_reqs = episode_interactions[ms.METRIC_REQUEST_COUNT_PARSED][1:]\n",
    "        v_reqs = episode_interactions[ms.METRIC_REQUEST_COUNT_VIOLATED][1:]\n",
    "        n_turns = len(reqs)\n",
    "\n",
    "        for turn in range(0, played_turns):\n",
    "            self.log_turn_score(turn, ms.METRIC_REQUEST_COUNT, reqs[turn])\n",
    "            self.log_turn_score(turn, ms.METRIC_REQUEST_COUNT_PARSED, p_reqs[turn])\n",
    "            self.log_turn_score(turn, ms.METRIC_REQUEST_COUNT_VIOLATED, v_reqs[turn])\n",
    "\n",
    "        aborted = int(episode_interactions[ms.METRIC_ABORTED])\n",
    "        lose = int(episode_interactions[ms.METRIC_LOSE]) if not aborted else 0\n",
    "        success =  1 - lose if not aborted else 0\n",
    "        bench_score = complete_turns / n_turns if not aborted else np.nan\n",
    "\n",
    "        self.log_episode_score(ms.METRIC_ABORTED, aborted)\n",
    "        self.log_episode_score(ms.METRIC_LOSE, lose)\n",
    "        self.log_episode_score(ms.METRIC_SUCCESS, success)\n",
    "        self.log_episode_score(ms.METRIC_REQUEST_COUNT, sum(reqs))\n",
    "        self.log_episode_score(ms.METRIC_REQUEST_COUNT_PARSED, sum(p_reqs))\n",
    "        self.log_episode_score(ms.METRIC_REQUEST_COUNT_VIOLATED, sum(v_reqs))\n",
    "        self.log_episode_score(ms.METRIC_REQUEST_SUCCESS, sum(p_reqs) / sum(reqs))\n",
    "        self.log_episode_score(ms.BENCH_SCORE, bench_score)"
   ],
   "id": "8686ab2f61ceb83b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NOTE: The `GameScorer` base class has methods to handle most of the above, so this is subject to change.",
   "id": "b2d2fbbe617c0378"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Game registry\n",
    "Every clemgame needs a game registry entry, stored as `clemgame.json` in the game's root directory. It is required for the clemcore framework to find the game and its file system path.\n",
    "\n",
    "The contents for firstlast are:\n",
    "```json\n",
    "{\n",
    "  \"game_name\": \"firstlast\",\n",
    "  \"description\": \"Firstlast game between two players that must match the first letters of the first and last word of their responses.\",\n",
    "  \"main_game\": \"firstlast\",\n",
    "  \"players\": \"two\",\n",
    "  \"image\": \"none\",\n",
    "  \"languages\": [\"en\"],\n",
    "  \"benchmark\": []\n",
    "}\n",
    "```\n",
    "Create a JSON file in the `firstlast` directory and copy the game registry entry into it."
   ],
   "id": "83b1bb50fe515441"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# GameBenchmark implementation\n",
    "\n",
    "We need to define which classes the framework needs to run our game. This can be done in the same file where the GameMaster lives. We need to create a child of `GameBenchmark`. We also add a short description of the game and the method that calls the FirstLast game master.\n"
   ],
   "id": "c9cca10af0d25e23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# put this at the end of games/firstlast/master.py\n",
    "\n",
    "# always add the GameBenchmark child with this structure\n",
    "class FirstLastGameBenchmark(GameBenchmark):\n",
    "    \"\"\"Integrate the game into the benchmark run.\"\"\"\n",
    "    def __init__(self, game_spec: GameSpec):\n",
    "        super().__init__(game_spec)\n",
    "\n",
    "    def create_game_master(self,\n",
    "                           experiment: Dict,\n",
    "                           player_models: List[Model]\n",
    "                           ) -> GameMaster:\n",
    "        return FirstLast(self.game_name, self.game_path, experiment, player_models)\n",
    "\n",
    "    def create_game_scorer(self, experiment: Dict, game_instance: Dict) -> GameScorer:\n",
    "        return FirstLastScorer(self.game_name, experiment, game_instance)"
   ],
   "id": "20abaab7043e7015"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
